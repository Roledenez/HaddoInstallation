How to remove OpenJDK and install Oracle JDK
https://support.cafex.com/hc/en-us/articles/200874471-How-to-remove-OpenJDK-and-install-Oracle-JDK

java -version
yum list java*
yum -y remove java*
java -version
wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie"  http://download.oracle.com/otn-pub/java/jdk/8u151-b12/e758a0de34e24606bca991d704f6dcbf/jdk-8u151-linux-x64.rpm

rpm -ivh jdk-8u151-linux-x64.rpm
java -version


generating SSH key
ssh-keygen -t rsa -P ''
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


download and install hadoop
 wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz
 tar xfz hadoop-2.9.0.tar.gz
mv hadoop-2.9.0 /opt/
ln -s hadoop-2.9.0 hadoop

update-alternatives --config java
/usr/java/jdk1.8.0_151/jre/bin/java

edit the ~/.bashrc file

#Hadoop Variables Start
export JAVA_HOME=/usr/java/jdk1.8.0_151
export HADOOP_INSTALL=/opt/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"
#Hadoop Variables End

in order to permenent the barshrc
source ~/.bashrc

open the hadoop-env.sh
vim /opt/hadoop/etc/hadoop/hadoop-env.sh
add java_home path

vim /opt/hadoop/etc/hadoop/core-site.xml

<configuration>
<property>
 <name>fs.default.name</name>
 <value>hdfs://localhost:9000</value>
</property>
</configuration>


 sudo vim /opt/hadoop/etc/hadoop/yarn-site.xml

<property>
 <name>yarn.nodemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
</property>
</property>
 <name>yarn.nodemanager.aux-services.mapredce.shuffle.clss</name>
 <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>


create and edit
mapred-site.xml
sudo cp /opt/hadoop/etc/hadoop/mapred-site.xml.template /opt/hadoop/etc/hadoop/mapred-site.xml
sudo vim /opt/hadoop/etc/hadoop/mapred-site.xml
<property>
 <name>mapreduce.framework.name</name>
 <value>yarn</value>
</property>

editing HDFS-site.xml
before that create the following folders
sudo mkdir -p /opt/hadoop_store/hdfs/namenode
sudo mkdir -p /opt/hadoop_store/hdfs/datanode

sudo vim /opt/hadoop/etc/hadoop/hdfs-site.xml

<property>
 <name>dfs.replication</name>
 <value>1</value>
</property>
<property>
 <name>dfs.namenode.name.dir</name>
 <value>file:/opt/hadoop_sotre/hdfs/namenode</value>
</property>
<property>
 <name>dfs.datanode.data.dir</name>
 <value>file:/opt/hadoop_store/hdfs/datanode</value>
</property>


hdfs namenode -format #create the namenode for given directory

to start try below
start-dfs.sh
start-yarn.sh



